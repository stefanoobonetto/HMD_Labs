{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fbfcbbe-7b3a-4389-bf3f-07a97c98d17b",
   "metadata": {},
   "source": [
    "# Generation with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3364287e-938c-4b06-9ff8-3de282d400be",
   "metadata": {},
   "source": [
    "Import the required libraries and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0a617-fe82-480d-978a-7dad80c620b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module '_sqlite3'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from typing import Tuple\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BatchEncoding, PreTrainedTokenizer, PreTrainedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2ba22-bd98-4463-a846-dbd19d1a0495",
   "metadata": {},
   "source": [
    "We will use ```Llama2-7b-chat``` and ```Llama-3-8B-Instruct```.\n",
    "These models are fine-tuned versions of the base models.\n",
    "Since the models were prompted with specific templates during fine-tuning, we will use the same templates to have the models be in the best conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff04792-e2fc-4c9a-8f92-766193c5ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"llama2\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"llama3\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "}\n",
    "\n",
    "TEMPLATES = {\n",
    "    \"llama2\": \"<s>[INST] <<SYS>>\\n{}\\n<</SYS>>\\n\\n{} [/INST]\",\n",
    "    \"llama3\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d8215-8964-4723-ad56-81603d5091fd",
   "metadata": {},
   "source": [
    "Functions for loading the models and generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f507a-a643-4c5f-b049-225846e146be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str, dtype) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "    torch_dtype = torch.float32\n",
    "    if dtype == \"bf16\":\n",
    "        torch_dtype = torch.bfloat16 \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch_dtype,\n",
    "\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate(\n",
    "    model: PreTrainedModel,\n",
    "    inputs: BatchEncoding,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_seq_length: int,    \n",
    ") -> str:\n",
    "    output = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_seq_length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(\n",
    "        output[0][len(inputs.input_ids[0]) :], skip_special_tokens=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f84fca-854a-411d-b00f-4f105e039687",
   "metadata": {},
   "source": [
    "Parameters and input for the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f467dec-07e5-409f-bff1-5a5dcfa2b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama2\"\n",
    "chat_template = TEMPLATES[model_name]\n",
    "model_name = MODELS[model_name]\n",
    "\n",
    "dtype = \"f32\" #\"bf16\"\n",
    "max_seq_length = 128\n",
    "\n",
    "system_prompt = \"You are a pizza ordering assistant.\"\n",
    "input = \"User: Hello, I would like a pizza. System: \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d3a6f-64d3-46d9-a56f-554c24d76695",
   "metadata": {},
   "source": [
    "Load the model and tokenizer based on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb702d-26e1-4d3d-a475-746efbc58914",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(model_name, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be1ff5-fdbd-42e6-aeb4-9074303d1ca4",
   "metadata": {},
   "source": [
    "Prepare the input and generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a5a0b3-6252-45b1-8d1a-731778e71ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and tokenize the input\n",
    "input_text = chat_template.format(system_prompt, input)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate a response\n",
    "response = generate(model, inputs, tokenizer, max_seq_length)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d474c-28ce-4d6d-b937-ae280e1df692",
   "metadata": {},
   "source": [
    "# Run on Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea3e14-4bc1-48dc-94f9-204961e975c7",
   "metadata": {},
   "source": [
    "* Connect to the [university VPN](https://servicedesk.unitn.it/sd/it/service/vpn?id=unitrento_v2_service_card&table=sc_cat_item&sys_id=e5a898fec35bfd104cbb7055df013171).\n",
    "\n",
    "* Connect to the cluster:\n",
    "\n",
    "    ```ssh firstname.lastname@marzola.disi.unitn.it```\n",
    "\n",
    "* Copy sbatch file to request resources and run:\n",
    "\n",
    "    ```cp /data/hmd_2024/example.sbatch .```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136fa77-f0c3-4590-9a5a-b9828bc053e9",
   "metadata": {},
   "source": [
    "#### example.sbatch\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --gres=gpu:1            # request 1 gpu\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=2       # request 2 cpus\n",
    "#SBATCH -N 1\n",
    "\n",
    "#SBATCH --job-name=hmd_example  # job name to be displayed in the queue\n",
    "#SBATCH --output=%x-%j.out      # output file name\n",
    "#SBATCH --error=%x-%j.err\n",
    "\n",
    "#SBATCH --partition edu5        # partition\n",
    "#SBATCH -t 0:5:0                # request 5 minutes\n",
    "\n",
    "/data/hmd_2024/run \"$@\"         # all parameters given to 'example.sbatch' are passed to '/data/hmd_2024/run'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dbf4c5-6844-4477-a255-65519754d516",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e02e13-e9d6-4033-ae7e-b1b9b8e40c53",
   "metadata": {},
   "source": [
    "Parameters for the underlying python script:\n",
    "\n",
    "```\n",
    "usage: python -m query_model [-h] [--system-prompt SYSTEM_PROMPT] [--dtype {f32,bf16}] [--max_seq_length MAX_SEQ_LENGTH] [--return-full] [--dotenv-path DOTENV_PATH] {llama2,llama3} INPUT_TEXT\n",
    "\n",
    "Query a specific model with a given input.\n",
    "\n",
    "positional arguments:\n",
    "  {llama2,llama3}       The model to query.\n",
    "  INPUT_TEXT            The input to query the model with.\n",
    "\n",
    "options:\n",
    "  -h, --help            show this help message and exit\n",
    "  --system-prompt SYSTEM_PROMPT\n",
    "                        The system prompt to use for the model. (default: )\n",
    "  --dtype {f32,bf16}    The data type to use for the model. (default: f32)\n",
    "  --max_seq_length MAX_SEQ_LENGTH\n",
    "                        The maximum sequence length to use for the model. (default: 128)\n",
    "  --return-full         Return the full output. (default: False)\n",
    "  --dotenv-path DOTENV_PATH\n",
    "                        The path to the .env file. (default: .env)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55a968-95c1-4d21-98bc-12aa1cb70297",
   "metadata": {},
   "source": [
    "#### Run model to generate a response\n",
    "Use ```sbatch``` to request a job on the cluster.\n",
    "All the settings for the job are in ```example.sbatch```.\n",
    "The other parameters will be passed to the python code.\n",
    "\n",
    "```sbatch example.sbatch --system-prompt \"You are a pizza ordering assistant.\" llama2 \"User: Hello, I would like a pizza System: \"```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
